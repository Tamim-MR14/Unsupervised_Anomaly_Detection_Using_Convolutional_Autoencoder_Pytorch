{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "# test_data = datasets.MNIST(root='data', train=False,\n",
    "#                                   download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training and test dataloaders\n",
    "\n",
    "# num_workers = 0\n",
    "# # how many samples per batch to load\n",
    "# batch_size = 20\n",
    "\n",
    "# # prepare data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# regtarget=1\n",
    "# anomalytarget=3\n",
    "# contam=0.01\n",
    "# seed=42\n",
    "\n",
    "\n",
    "# regIdxs = np.where(targets == regtarget)[0]\n",
    "# anomalyIdxs = np.where(targets == anomalytarget)[0]\n",
    "\n",
    "# # randomly shuffle both sets of indexes\n",
    "# np.random.seed(42)\n",
    "# np.random.shuffle(regIdxs)\n",
    "# np.random.shuffle(anomalyIdxs)\n",
    "\n",
    "# # compute the total number of anomaly data points to select\n",
    "# i = int(len(regIdxs) * contam)\n",
    "# anomalyIdxs = anomalyIdxs[:i]\n",
    "\n",
    "# # use NumPy array indexing to extract both the valid images and\n",
    "# # \"anomlay\" images\n",
    "# regImages = data[regIdxs]\n",
    "# anomalyImages = data[anomalyIdxs]\n",
    "\n",
    "# # stack the valid images and anomaly images together to form a\n",
    "# # single data matrix and then shuffle the rows\n",
    "# images = np.vstack([regImages, anomalyImages])\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unsupervised_dataset(data, targets, regtarget=1,\n",
    "    anomalytarget=3, contam=0.01, seed=42):\n",
    "    # grab all indexes of the supplied class label that are *truly*\n",
    "    # that particular label, then grab the indexes of the image\n",
    "    # labels that will serve as our \"anomalies\"\n",
    "    regIdxs = np.where(targets == regtarget)[0]\n",
    "    anomalyIdxs = np.where(targets == anomalytarget)[0]\n",
    "\n",
    "    # randomly shuffle both sets of indexes\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(regIdxs)\n",
    "    np.random.shuffle(anomalyIdxs)\n",
    "\n",
    "    # compute the total number of anomaly data points to select\n",
    "    i = int(len(regIdxs) * contam)\n",
    "    anomalyIdxs = anomalyIdxs[:i]\n",
    "\n",
    "    # use NumPy array indexing to extract both the valid images and\n",
    "    # \"anomlay\" images\n",
    "    regImages = data[regIdxs]\n",
    "    anomalyImages = data[anomalyIdxs]\n",
    "\n",
    "    # stack the valid images and anomaly images together to form a\n",
    "    # single data matrix and then shuffle the rows\n",
    "    images = np.vstack([regImages, anomalyImages])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(images)\n",
    "\n",
    "    # return the set of images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data.data.cpu().numpy()\n",
    "targets = train_data.targets.cpu().numpy()\n",
    "\n",
    "images = create_unsupervised_dataset(data,targets)\n",
    "\n",
    "X_train, X_valid = train_test_split(images, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE CUSTOM PyTorch DATSET CLASS FOR OUT PARTICULAR DATSET ###\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "class MNISTAnomalyDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = np.uint8(X)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "             sampleX = self.transform(self.X[idx])\n",
    "\n",
    "        sample = (sampleX)\n",
    "\n",
    "\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "data = {}\n",
    "\n",
    "data['train'] = MNISTAnomalyDataset(X_train, transform=transform)\n",
    "data['valid'] = MNISTAnomalyDataset(X_valid, transform=transform)\n",
    "\n",
    "loaders = {}\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "loaders['train'] = torch.utils.data.DataLoader(data['train'], batch_size=32, num_workers=0, shuffle=True)\n",
    "loaders['valid'] = torch.utils.data.DataLoader(data['valid'], batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23b8cc639c8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(loaders['train'])\n",
    "images = dataiter.next()\n",
    "images = images.numpy()\n",
    "print(images.shape)\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoEncoder(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 32), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  \n",
    "        # conv layer (depth from 32 --> 16), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        # conv layer (depth from 16 --> 8), 3x3 kernels\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        # transpose layer, a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(8, 8, 3, stride=2)  # kernel_size=3 to get to a 7x7 image output\n",
    "        # two more transpose layers with a kernel of 2\n",
    "        self.t_conv2 = nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(16, 32, 2, stride=2)\n",
    "        # one, final, normal conv layer to decrease the depth\n",
    "        self.conv_out = nn.Conv2d(32, 1, 3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # add third hidden layer\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        # transpose again, output should have a sigmoid applied\n",
    "        x = F.sigmoid(self.conv_out(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoEncoder()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_transfer = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(data)\n",
    "            loss = criterion(output,data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data) in enumerate(loaders['valid']):\n",
    "                # move to GPU\n",
    "                if use_cuda:\n",
    "                    data = data.cuda()\n",
    "                ## update the average validation loss\n",
    "                \n",
    "                output = model.forward(data)\n",
    "                loss = criterion(output,data)\n",
    "                \n",
    "                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            \n",
    "            \n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\dlnanodegree\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.094278 \tValidation Loss: 0.065671\n",
      "Validation loss decreased (inf --> 0.065671).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.065990 \tValidation Loss: 0.065669\n",
      "Validation loss decreased (0.065671 --> 0.065669).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.066097 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065669 --> 0.065668).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.066068 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.066076 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.066073 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.066088 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.066022 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.065990 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.066032 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.066059 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.066035 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.066064 \tValidation Loss: 0.065668\n",
      "Epoch: 14 \tTraining Loss: 0.066047 \tValidation Loss: 0.065668\n",
      "Epoch: 15 \tTraining Loss: 0.065997 \tValidation Loss: 0.065668\n",
      "Epoch: 16 \tTraining Loss: 0.066040 \tValidation Loss: 0.065668\n",
      "Epoch: 17 \tTraining Loss: 0.066074 \tValidation Loss: 0.065668\n",
      "Epoch: 18 \tTraining Loss: 0.066022 \tValidation Loss: 0.065668\n",
      "Epoch: 19 \tTraining Loss: 0.066064 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.066075 \tValidation Loss: 0.065668\n",
      "Epoch: 21 \tTraining Loss: 0.066062 \tValidation Loss: 0.065668\n",
      "Epoch: 22 \tTraining Loss: 0.066032 \tValidation Loss: 0.065668\n",
      "Epoch: 23 \tTraining Loss: 0.066100 \tValidation Loss: 0.065668\n",
      "Epoch: 24 \tTraining Loss: 0.066033 \tValidation Loss: 0.065668\n",
      "Epoch: 25 \tTraining Loss: 0.066089 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.066055 \tValidation Loss: 0.065668\n",
      "Epoch: 27 \tTraining Loss: 0.066010 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.066004 \tValidation Loss: 0.065668\n",
      "Validation loss decreased (0.065668 --> 0.065668).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.062208 \tValidation Loss: 0.029054\n",
      "Validation loss decreased (0.065668 --> 0.029054).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.012165 \tValidation Loss: 0.008278\n",
      "Validation loss decreased (0.029054 --> 0.008278).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.007088 \tValidation Loss: 0.006743\n",
      "Validation loss decreased (0.008278 --> 0.006743).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.006207 \tValidation Loss: 0.006173\n",
      "Validation loss decreased (0.006743 --> 0.006173).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.005791 \tValidation Loss: 0.005869\n",
      "Validation loss decreased (0.006173 --> 0.005869).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.005441 \tValidation Loss: 0.005502\n",
      "Validation loss decreased (0.005869 --> 0.005502).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.005161 \tValidation Loss: 0.005179\n",
      "Validation loss decreased (0.005502 --> 0.005179).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.004976 \tValidation Loss: 0.005160\n",
      "Validation loss decreased (0.005179 --> 0.005160).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.004789 \tValidation Loss: 0.004876\n",
      "Validation loss decreased (0.005160 --> 0.004876).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.004628 \tValidation Loss: 0.004701\n",
      "Validation loss decreased (0.004876 --> 0.004701).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.004541 \tValidation Loss: 0.004767\n",
      "Epoch: 40 \tTraining Loss: 0.004406 \tValidation Loss: 0.004587\n",
      "Validation loss decreased (0.004701 --> 0.004587).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.004316 \tValidation Loss: 0.004429\n",
      "Validation loss decreased (0.004587 --> 0.004429).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.004222 \tValidation Loss: 0.004363\n",
      "Validation loss decreased (0.004429 --> 0.004363).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.004194 \tValidation Loss: 0.004308\n",
      "Validation loss decreased (0.004363 --> 0.004308).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.004067 \tValidation Loss: 0.004308\n",
      "Epoch: 45 \tTraining Loss: 0.004024 \tValidation Loss: 0.004234\n",
      "Validation loss decreased (0.004308 --> 0.004234).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.003973 \tValidation Loss: 0.004202\n",
      "Validation loss decreased (0.004234 --> 0.004202).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.003943 \tValidation Loss: 0.004092\n",
      "Validation loss decreased (0.004202 --> 0.004092).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.003891 \tValidation Loss: 0.004438\n",
      "Epoch: 49 \tTraining Loss: 0.003859 \tValidation Loss: 0.004017\n",
      "Validation loss decreased (0.004092 --> 0.004017).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.003795 \tValidation Loss: 0.003956\n",
      "Validation loss decreased (0.004017 --> 0.003956).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.003748 \tValidation Loss: 0.003937\n",
      "Validation loss decreased (0.003956 --> 0.003937).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.003711 \tValidation Loss: 0.004040\n",
      "Epoch: 53 \tTraining Loss: 0.003681 \tValidation Loss: 0.003819\n",
      "Validation loss decreased (0.003937 --> 0.003819).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.003632 \tValidation Loss: 0.003836\n",
      "Epoch: 55 \tTraining Loss: 0.003612 \tValidation Loss: 0.003775\n",
      "Validation loss decreased (0.003819 --> 0.003775).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.003580 \tValidation Loss: 0.003785\n",
      "Epoch: 57 \tTraining Loss: 0.003589 \tValidation Loss: 0.003726\n",
      "Validation loss decreased (0.003775 --> 0.003726).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.003524 \tValidation Loss: 0.003721\n",
      "Validation loss decreased (0.003726 --> 0.003721).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.003555 \tValidation Loss: 0.003649\n",
      "Validation loss decreased (0.003721 --> 0.003649).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.003479 \tValidation Loss: 0.003698\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 60\n",
    "model_transfer =  train(n_epochs, loaders, model, optimizer, criterion, use_cuda, 'conv_Autoencoder_anomaly_adam_lr0001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXEAAADrCAYAAAAv1NW3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debRW1Xk44HOZB1GRQRwAJwQxjmgFCYpJmmapiRMQjZG4spbGpNUK2sRo45ClriQ1appijda0Gk2M4tCYxaq2KA5LwClqFcEhikgUAZlk5nJ/f/yWO3vvcK9wy/fd8333ef56377fxbdl95zzbc5+b0NTU1MBAAAAAEA5dWjrBgAAAAAAaJ5NXAAAAACAErOJCwAAAABQYjZxAQAAAABKzCYuAAAAAECJ2cQFAAAAACixTtvy4YaGhqZKNcI2W9LU1NSvrZvYGtZNeTQ1NTW0dQ9bw5opFdcaWsO6oTWsG1rDuqE1rBtaw7phm/kOTis0e63xJm7tmt/WDQDtgmsNrWHd0BrWDa1h3dAa1g2tYd0A1dDstcYmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDEbOICAAAAAJSYTVwAAAAAgBKziQsAAAAAUGI2cQEAAAAASswmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDEOrV1A2X205/+NMnHjx8f4tGjRye1BQsWVKUnoLz69esX4kcffTSpfeYzn0nys88+O8S33357RfsCgKIoioEDB4b46quvTmoTJ05M8mnTpoX4y1/+clLbvHlzBbqjFvz85z9P8u985ztJ/sQTT4T4pJNOSmorV66sXGO0uWOPPTbJZ8yYEeLf//73SS2/psD/1ZVXXpnkV1xxRYiPO+64pBavTcppwoQJSf7b3/42xPk+3T/8wz+EuKmpqbKNlYA3cQEAAAAASswmLgAAAABAidnEBQAAAAAoMTNxWzB58uRma3vssUeSm4lLc4455pgQ33zzzUlt6NChIR4wYEBSW7x4cWUbY7uL5/EMHz48qa1ZsybJf/e731WlJ+rLyJEjQzxz5syk9tWvfjXE99xzT9V6om3stddeIc7/vo888sgkv+mmm0IcX6eK4i+vTdSX7t27J/k3v/nNEH/9619Pavmc2y996Ushzp+Jr7vuuu3VIjXm5JNPTvJ8/uCYMWNC/MUvfjGpTZ06tXKN0eZOOeWUJDc7m2rKZzLHxo4dm+Rm4pbfww8/nOSrVq0Kcf5M8uyzz4Y4np1br7yJCwAAAABQYjZxAQAAAABKzDiFzIQJE5qtxSMTZs2aVY12qEH9+vVL8p/+9KchjscnFEV6BC2vGadQe+Kj7rnvf//7Sb5s2bJKt0MdGjVqVFu3QBvJ7y2//OUvQ3zYYYcltfwI67nnnhvi66+/Pqm99dZb26tFSqJTpz8/3uejNo4//vgQb9q0aav/nJbub9S/v/3bvw3x7rvvntTycQoA1XLllVeGOB+ZQG1bsWJFkr/33nshzvdN4lEuxikAAAAAANCmbOICAAAAAJSYTVwAAAAAgBIzEzdz1FFHNVszB5fmDBs2LMRz5sxJavGssDVr1iS1uXPnhvipp56qUHdUyr777pvkRxxxRLOfzWf3QGuMHz8+xDNnzkxq+exL6stOO+2U5GPGjNnqn7366qtD/O677263niiHeHZtURTFpZdeGuJ4Bm5RFMXbb78d4osuuiipTZo0Kcm3ZY1R384777yt/uzKlStD7LsT0FZmzJgR4nh2LrVp2rRpIY73XoqiKE466aQQjxgxIqk9//zzlW2sDXgTFwAAAACgxGziAgAAAACUWLsfpzBw4MAknzx5crOfXbBgQaXboUYcc8wxSX777beHOB6fkOcTJ05Mag888EAFuqNaOnfunOTdunULcb4ONm/eXJWeqC8TJkxI8lGjRoX4+uuvr3Y71KglS5aEeOPGjW3YCZWwzz77JPkVV1wR4g0bNiS1KVOmhPg///M/k1o+TgFaY/369SF+77332rATymTw4MEt5vPnz69mO9SJY489ttna448/XsVOqLSW9uK6dOkS4rPPPjupGacAAAAAAEBV2cQFAAAAACgxm7gAAAAAACXW7mfixvMFP829995bwU6oJSNGjEjyQYMGhbihoSGp3XLLLSE2A7e+5XNw4f/qqKOOarY2e/bsKnYC1KKPP/44yV988cU26oR6kT/nbmud9mn48OFJPmzYsCQ3E5etMXbs2Bbz2JVXXlnRXqiuhx9+OMQtfef+xje+keS33XZbktfDc5A3cQEAAAAASswmLgAAAABAibX7cQoXXnhhs7V8fMKsWbMq3Q414pJLLkny+JX+JUuWJLVbb721Kj1RfQsWLEjyOXPmhDg/Nnb88ccn+fnnn1+5xqhZAwcOTPLJkycnebzmZs6cWZWeKKcOHZr/d/j4yFlRFMWUKVMq3Q5t6P3330/y+Pn1S1/6UlK7+uqrQzx69OjKNkZdyo+xfloOW3LeeecleX7fgi254oormq3NmDGjeo1QdfF3oPg7d1Gk37t79uyZ1L773e8m+de+9rUKdFdd3sQFAAAAACgxm7gAAAAAACVmExcAAAAAoMTa3UzckSNHJvmoUaOa/ax5g+1bPE/ljjvuSGr9+vVL8nj+1yOPPJLUXnjhhQp0RxmsXr06yZctW9bsZ/fYY49Kt0MdaOmeVBTprMt8JjPty+bNm5utNTY2VrET2tqqVauS/PTTTw/xUUcdldS6devWqv9GPncXIHf//fcneUu//2HYsGGVboc6NHbs2GZrV111VfUaoeri790tfefO1eN3cG/iAgAAAACUmE1cAAAAAIASa3fjFD7tqGps6tSpFeyEsjvllFNCfNJJJyW1eHxCURTFnDlzQnzWWWdVtjFKq6GhoVU1+MS4ceNarN94441V6gSoB7Nnz271zy5fvjzEP//5z7dHO0Ade+KJJ5K8Q4c/vy+WjwCKa9CcK6+8ssV6PEJhxowZlW2G0njjjTeSfPTo0c1+dt68eZVup+pcPQEAAAAASswmLgAAAABAidnEBQAAAAAosXY3E3fSpEkt1u+9994QL1iwoNLtUGKXXnppiPN5pnn+ox/9qCo9UW5LlixptpbPUYZPjBw5MsTjx49PajNnzkxy96X2K5/N3pLf//73FeyEerHzzjsn+U477ZTkTz/9dIhff/31qvRE7bv77rvbugVKIp6Dmz8H5zNy4RNjx44N8RVXXNF2jVBaP/nJT5J84sSJIc7nbQ8dOrQqPVWTN3EBAAAAAErMJi4AAAAAQIm1i3EKEyZMCPHAgQNb/Gx+dJX6NmzYsBCfdtppSS1+9T4/AvTAAw+0mNM+PfjggyH+yle+ktQ6duyY5PH6mjdvXmUbo9RGjRrVbC0e8UP7du655271Z3/xi19UsBPqRT5OIc/fe++9arZDifTs2TPJu3XrFuL8qGp+LH758uWVa4xSGzduXFu3QB147LHHmq3NmDEjya+88srKNkMp5d+d4/tQfo+qR/X/vyEAAAAAQA2ziQsAAAAAUGI2cQEAAAAASqxdzMQ96qijmq0tWLAgyadOnVrpdiiReObXBRdckNQaGhpCvGTJkqT2j//4j0m+Zs2aCnRHrYnXTC6fzzNy5MgQm4nbvo0fPz7E7kk054QTTkjylq4b06ZNS/Ljjz++Ij1R2wYPHpzkgwYNSvJXXnmlmu1QIkceeWSS77333iHOZ+DmvzfizTffrFxjlJpnFlpj7NixW/3Z4447rnKNULNa+g7eUq1WeRMXAAAAAKDEbOICAAAAAJSYTVwAAAAAgBKry5m48azJoiiKyZMnN/vZWbNmJXk+j5D6ds4554S4T58+SS2e8XXttdcmtblz51a2MWpSPhdue32W+jJhwoQkHzVqVIivv/76pOaeRHPyuZSxxsbGKnYC1Jvu3bu3+meHDBkS4q5duya19evXt/rPpfbEvw8iv2f17t07yQ855JAQv/TSS5VtjFJ57LHHmq3NmDGjeo1Qs15++eUQH3bYYUmtY8eOzea1+rzsTVwAAAAAgBKziQsAAAAAUGJ1OU4hPpr6afKjq9S3YcOGJfm5554b4vx4+9KlS0NsfAKwvYwbNy7J45EJN954Y7XbAdiuvvzlL4e4Z8+eSS1+nnrxxRer1hOfbqeddgrxzTff3Oo/53vf+16I82PSjz/+eKv/XGpPPEIh/57Vt2/fJB8zZkyIjVPgE64ZbI3Zs2eHOB+nkI9a3W+//UI8b968yjZWId7EBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDE6mYm7sCBA0Pc0pzbmTNnJvmsWbMq1hPlc9pppyV5PJ8pn9X0i1/8IsRPPvlkZRujLjQ0NGwx/rTPUv/ie9T48eOT2r333hvieD4uQCU988wzSf7cc89t9c9+5StfCfH3v//9pHb44YeHuFOn9KtGPPfbTNxy6dq1a4j32GOPVv85y5cvD7F5lsCW5POyYzNmzNhiDM3Zlu/g9cCbuAAAAAAAJWYTFwAAAACgxOpmnMKoUaO26nP5OAXq34gRI0J8wQUXJLWWXre/6667QrxmzZrt3xh1p6XxHC19lvo3bty4ZmvuS0BbWLt2bYv52LFjQ/z2228ntd122y3EnTt3Tmrz5s0L8c9+9rOkds8997SqVyrvsssuC3FLz8cdOqTvAD3//PNJPmbMmO3bGFDz4vvJlvJYPIYl/5zxCmzJtnwH32uvvUIcP6/UEm/iAgAAAACUmE1cAAAAAIASs4kLAAAAAFBidTMTt6V5g7Ebb7yxwp1QNvFsrj59+iS1eGbKnDlzktrcuXMr2xh1Z+XKlc3W8vlyLc2bo/5MmjQpxPkM3BtuuKHa7VCD3nzzzSS/6KKLQpyvoRNPPDHJ/+7v/i7E//Iv/1KB7iirfv36JXn8vPz1r389qcW/Q6Ao0lm3gwYNSmp33313iK+55pqktnDhwhCvWLFiGzumWgYOHJjkEydODHFLMwWXL1+e5JdcckmS57OVab+efPLJEJuV3L61NAM3d+yxx4b4uOOOq0A31Jv4dxl9+9vfbvGzn//850P88MMPV6ynSvImLgAAAABAidnEBQAAAAAosZodp5AfARo/fnyzn42Pri5YsKBiPVFOBxxwQIjzI+xLly4N8cUXX1y1nqhPDzzwQIgbGxuTWseOHZN8n332qUpPtI0JEyYkeXzPMj6B7SE+7rx58+at/iy1Iz5+euihh271z5166qkhPuigg5LajjvuuNV/TksjE+bNmxfi/H5HbRg5cmSS9+rVa6t+7uqrr07y//mf/9luPVFfrr/++hAffvjhSa1Hjx5JPnTo0Kr0RNuIRyTA9rZu3boQ58+89TjC0Ju4AAAAAAAlZhMXAAAAAKDEbOICAAAAAJRYzc7EHTVq1FZ/Np6JS/075ZRTkvzkk08OcT4jJc5fe+21yjZGu/LRRx8lef/+/ZP80ksvDfFdd92V1F5//fXKNUZVXHfddUke34fMxGV7uOOOO0Kc3/fMnqsPJ510UogvuOCC7fJn/ulPfwrxnXfemdR+9atfJbm5t2zJmjVr2roFasTvfve7EL/00ktJLf8u/+1vfzvE559/fmUbo+riGe+f5rjjjqtcI9SleCZuHBdFUXTv3j3Jd9lll6r0VEnexAUAAAAAKDGbuAAAAAAAJVaz4xTGjRu31Z+98cYbK9gJZXPqqacmeXyMffPmzUlt7dq1IXY8jO3pxBNPTPJp06Yl+fTp00O8YMGCqvREZU2YMCHEAwcObLYG28OKFStCvHr16jbshEp55JFHQpw/v+y3334hHjx4cFJ74403Qvzoo48mtVtvvTXEmzZt2i59UpveeuutJI+fifPjp7/5zW9C/Oqrr1a2MerStddem+QPPfRQkk+dOrWa7VBlM2bMSPJ4vILxCfxfzZkzJ8QPPvhgUjv99NOT/MADD6xKT5XkTVwAAAAAgBKziQsAAAAAUGI2cQEAAAAASqyhqalp6z/c0LD1H6bSnm9qajqirZvYGtVeN6ecckqSxzOW4nkpRVEUl19+eYgfeOCByjZWAk1NTQ1t3cPWcK0pFdearRTPvc3ntrfDmbjWTRWNGDEiyW+66aYkHz9+fIjffffdqvTUStYNrWHd0BrWDa1h3bDNfAenFZq91ngTFwAAAACgxGziAgAAAACUmHEKtctRDraZoxy0gmsNrWHd0BrWDa1h3dAa1g2tYd2wzXwHpxWMUwAAAAAAqEU2cQEAAAAASswmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDEbOICAAAAAJSYTVwAAAAAgBKziQsAAAAAUGI2cQEAAAAASqzTNn5+SVEU8yvRCNtscFs3sA2sm3KwZmgN64bWsG5oDeuG1rBuaA3rhtawbthW1gyt0ey6aWhqaqpmIwAAAAAAbAPjFAAAAAAASswmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDEbOICAAAAAJSYTVwAAAAAgBKziQsAAAAAUGI2cQEAAAAASswmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZhMXAAAAAKDEbOICAAAAAJSYTVwAAAAAgBKziQsAAAAAUGI2cQEAAAAASqzTtny4oaGhqVKNsM2WNDU19WvrJraGdVMeTU1NDW3dw9awZkrFtYbWsG5oDeuG1rBuaA3rhtawbthmvoPTCs1ea7ZpE5dSmd/WDQDtgmsNrWHd0BrWDa1h3dAa1s120NDw572ppqZ2sf9j3QDV0Oy1xjgFAAAAAIASs4kLAAAAAFBixikAAAAA26SdjFAAKA1v4gIAAAAAlJhNXAAAAACAEjNOAQAAANgmDQ0NITZaAdhe4mtLUbi+xLyJCwAAAABQYjZxAQAAAABKzCYuAAAAAECJmYkLsJ106PDnfxfbZZddktqSJUuq3Q4AJOL7VK9evZLahg0bknzt2rVV6Yna0rlz5yTfvHlzkjc2NlazHUqke/fuSd6xY8cQr169OqmZb0mlxTNV8/mqsfwaRjnsuOOOSb5x48YQr1+/Pqm1t/uON3EBAAAAAErMJi4AAAAAQIkZp5Dp1OnP/yf59a9/ndSOOuqoEJ922mlJ7fnnnw+x4yE0p2/fvkkeHxN4++23k5p1VHsOPvjgEE+bNi2pvfXWW0k+ZsyYqvREfYmPg/Xs2TOpxUeh82PR1Ledd945yfNjg8uWLatmO5RIPD6hKIri8MMPD/GUKVOS2ogRI5I8Hqdw0EEHJbX58+eH2PNK/YufV3/zm98ktc9//vNJ/sorr4T4yCOPTGrWSn373Oc+l+Txd+nZs2cntS9+8YtV6Yn2Y9KkSUl+ySWXhPhHP/pRUovvf/Ex/aJwnSqLeF+uKIritttuC3Hv3r2T2mWXXRbiWbNmVbaxEvAmLgAAAABAidnEBQAAAAAoMZu4AAAAAAAlZiZuplevXiE+9NBDk9oOO+zQ7M+ZncIn8lmEF1xwQYivuOKKpLZy5coQn3766UmtPcxzqTd/9Vd/FeJddtklqS1atCjJ4zk/mzZtqmxj1I2hQ4eG+Mknn0xq06dPD/E3vvGNpLZ+/frKNkbVxc8o//qv/5rU9t9//yR/+umnQ3zqqacmtXwWHLUvfg7p1q1bUjvxxBNDnK+T/PmlR48eIZ46dWpSGzduXIjfe++9pOaeVn8GDRoU4vz7UT63cMiQISHed999k9qbb75Zge5oS/F34AEDBiS1+PozatSopNalS5ckN8uf1ujYsWOI85nMXbt2DXFL9yn7OOW0bt26JI+/Z+fXmuuvvz7EX/jCF5LamjVrKtBd2/ImLgAAAABAidnEBQAAAAAoMeMUMvERoZ49eya1+fPnh3ju3LlV64na0qdPnyS/+OKLQ5yvqQULFoT4nXfeqWhfVF58fCM/JpYfNzROgdY44ogjQrx69eqkdt9994XY+IT6d/nll4c4P94cHy8siqIYPnx4iPPRUMuWLatAd7Sl+AjpOeeck9TOOuusEO+8884t/jmNjY0h7tAhfe8jHsPhHlb/4jEs/fr1a/Gz8RHYeCQH9S8eS1gURbF58+ZmP9u9e/ckN06B1ujdu3eIDz744KQWjy184YUXklpLa5NyiJ9BiiL97pw/565duzbE7WE8hjdxAQAAAABKzCYuAAAAAECJ2cQFAAAAACixdj8TN5/xdcEFF4R4wIABSS2epbJmzZrKNkZNiWcM3n333Uktnh22ZMmSpDZ58uQQf/DBBxXqjkppaGhI8iFDhjRbi2f1FIUZgmydfI72d7/73RDns01nzpxZlZ5oG/mc7VGjRoW4W7duSS2f9RbPX1+1alUFuqNM4rWRP8vOmjWr2Vq+juK1snDhwqQWz+TO73ftYR5de5Z/d8pnE8Z//0uXLq1KT5TDihUrkjy/NsTy2cr5z8LWOP/880M8aNCgpPaHP/whxPFzELUh//0eTz/9dIhPOOGEpDZixIgQ77HHHkntzTffrEB3bcubuAAAAAAAJWYTFwAAAACgxNr9OIU999wzyT/72c+GeOPGjUntpptuCnF+VJH2pVOn9P91fvazn4U4PsZYFEWxbt26EF999dVJ7dFHH61Ad1RL9+7dkzw/mhrLj4Q0NjZWpCfqy7hx45J86NChIf6v//qvpPanP/2pKj3RNnr37p3kO+20U7Ofzce1xGN+jHKpP/mR5XfffTfEt956a1KLx3LsvvvuSW306NFJHq+Vl19+udma8Qn1b6+99gpxS0fkiyIdObd8+fJKtUQJbcv340MOOSTJ6/HIM9tfPmbs7LPPDnG+/qZMmRLiDRs2VLQvtr/82eLf/u3fQhzv2RVFUfTp0yfEl1xySVL71re+leT18B3cm7gAAAAAACVmExcAAAAAoMRs4gIAAAAAlFi7n4n7N3/zN0kezwr76KOPktqsWbOq0hPl07FjxyT/8Y9/nOSnn356iLt27ZrU4lmEt9xyS1IzR662DR8+PMl32GGHEOfzdj744IMk93fPluSzBvM5TqtXrw7xD37wg6RmVnt9y9dGnOfXk3zubT4/mfqSz+lfunRpiNeuXZvU4t/3kF9D7rnnniRfuXJliN97772k5npT3/Ln3vjvO7++xN+diqIoFi5cGGJzKNuX/DtQvI7ye9j+++9flZ6oLyNHjkzy+PoTz4MviqK48847q9IT1fHUU0+F+LXXXktqRx99dIhPO+20pHbzzTcn+XPPPVeB7qrLm7gAAAAAACVmExcAAAAAoMTa3TiF/HjQ1772tSTv0aNHiB955JGktmzZsso1RqntuOOOSX7SSSclebxulixZktQuvfTSEOdH7Klt23KcNB/PAlvSs2fPJN93332TPD7S/MYbb1SlJ8ohvs8URVF06PDnf4fPj6nmR5jjY/HUn/79+yd5fKR58eLFSS1+DjnjjDNa/HO6desW4l133TWprV+/vnXNUhPya8rQoUNDnH+Xyse5vP32283WqG/5M0wsXwv5mDHYkvxaFI8wLIp0lN0111yT1Nyn6kt878lHt8RjfuKxUUWRrpF64U1cAAAAAIASs4kLAAAAAFBiNnEBAAAAAEqs3c3E3XvvvZP8iCOOaPazU6dOrXQ7lFiXLl1CfNlllyW1wYMHJ3k8Y+6qq65KaosWLapAd5TBvHnzkjye77XPPvsktTyPZzyZGde+xWvhrLPOSmr9+vVL8htuuCHEa9eurWxjlMruu++e5C1dQ1asWJHkH3/8ceUao80tXLiwVT83bdq0JD/zzDOTPF5Xhx12WFKLZ9OZ919/OnfunOTxWsivN/nMyngu6rb87gBqX0vXok6d0m2H/JryH//xHyH2XMwn8u9P+UzcNWvWhPi2226rSk+0jfhZ9sUXX0xq8Z5e/ruMTjvttCR//PHHQ1yr1xpv4gIAAAAAlJhNXAAAAACAEmsX4xQ6dPjzXvXFF1+c1HbYYYckj1/TfuyxxyrbGKWSHwc744wzQpwfcc6PBP3xj38M8S233FKB7iijdevWJfn8+fNDnB//6dOnT5Ibp8AnevXqFeKLLrooqeXXpbvuuqsqPVE+Q4YMSfKWriHxPakoimLjxo2Va4ya9c477yR5fvS9W7duIY6PrBZF+hy0YcOG7d8cbSoeKVYURdG/f/8Q5/elfN107949xPF3sC19lvqybNmyJI//vvPvTvmIQ/hEfP259dZbk1q+d/Pwww+H2Jix+hZfT1577bWkFj8Hx+OeiqIohg4dWtnG2oA3cQEAAAAASswmLgAAAABAidnEBQAAAAAosXYxE3fQoEEhHjduXFLL58g988wzIf7www8r2xilks9muu6660Lcu3fvpBbPTi6Kojj++ONDbDZc+9HY2Jjkzz77bIjHjh2b1AYMGJDk8cy41atXb//mqBkTJ04McT5L+c0330zyhQsXVqUnyiGePZnPgcuvP7Hnn38+yc3dZkvy2abxDNyiSGdYvvrqq0nN7MH61rNnz1b/bDzn3Qzc9mXFihVJHn8n6ty5c1Lbf//9kzy+/ri+tG/nnXdeiD/72c8mtfz3kVx44YVV6Ym2Fz/LvvHGG0ktvtbk96/4nlQU6czcTZs2bc8Wq8abuAAAAAAAJWYTFwAAAACgxNrFOIUDDzwwxPnr1MuXL0/yn/zkJ1XpiXKIjxKeccYZSS0+RtihQ/rvHf/93/+d5G+//XYFuqPW7LnnniHOj6nmIxPiY2PGKbRv8bHB9evXJ7W77roryR2Lb1/iv++WRjzl96j82Sa+HllDfGKnnXZK8nxER3w8cfbs2UnNOqpv8fNMURRFv379Qpxfb/Kj72+99VaznzVeob599NFHSR5/l8qfi/M8HhlknEL7Eh9vL4qiGDx4cIjza8j06dOTfNGiRZVrjNJasGBBkrd0rcnXV5cuXUJsnAIAAAAAANudTVwAAAAAgBKziQsAAAAAUGJ1ORM3n3sxadKkEMfzMoqiKN5///0kf/LJJyvXGKWz8847h/iCCy5IavH85Hxm6bXXXpvk8dw42q8PPvggxPnct3gGblGka2/p0qWVbYxSiWcxFUVRnHfeeSFesmRJUvvlL3+Z5OZQtl/5TNz4GpOvi759+ya5mV//THUAAAyhSURBVLhsSf57IvK1MWfOnBD/8Y9/TGrxs3Y+S5fat88++yR5jx49mv1s/t2qT58+FemJ8lu1alWStzQDeccdd0zyXXfdNcSLFy/evo1RasOGDUvyCRMmhDi/v1xzzTVJvnHjxso1RmktW7YsyVt6tu3du3eSx/ezNWvWbN/GqsSbuAAAAAAAJWYTFwAAAACgxGziAgAAAACUWF3OxN17772T/MADDwxxPrv0iSeeSPJanYtB6/zwhz8Mcf/+/ZNavBb++Z//Oam99NJLlW2MmrRu3boQ57N5evbsmeT5LELaj9GjRyd5PBfun/7pn5JaPred9mvt2rVJHj/PdOiQ/pv8brvtluTxTFz4RL9+/ZL8f//3f5P8lltuCXE+l3DMmDEhfuqpp5JavN7MK6wd8XXkmGOOSWrx3Nt8zmn+vLNo0aIQm5fcvmzatCnJ4/tU/rsh8t8PkF+PqG/xXPX8e3b8XDxv3ryk9sILL1S2MWpCfm+Jrz35Papr165J3tKM91rhTVwAAAAAgBKziQsAAAAAUGJ1M04hfiX/8ssvT2oDBgwI8dKlS5PalClTKtsYbS4+1jdixIikduaZZzb7c4sXLw5xfsTZ8UCKoig6d+6c5B9++GGIV69endTy44b5UQ/qW3x0J79H7bLLLiH+9a9/ndRca/hEPjJh2bJlIe7bt29Si5978p91vLl9i9dCfES+KIrioYceSvLu3buHOD9+GF+rJk2alNTitTl9+vSkFh+hzkeE0LbiI6dDhgxJavG6yY/M5/ep5557LsT5sw/1LX+2feedd0J8wAEHJLX4u3tR/OW4BerbOeecE+Kjjz46qcXfr+68886kZvQlW7Jw4cIQ77XXXkktf9bJv7/XIm/iAgAAAACUmE1cAAAAAIASs4kLAAAAAFBidTMTN55tceihhya1eP5bPOe0KIpi/vz5lW2MNhfP8Ro/fnxS69WrV4jzuV1XXXVViJcvX16h7qhl+WzJeOZOPusrX1/5fEHq27Bhw0K86667JrUf/vCHIc7ntsMn1q1bl+QbNmwIcT4vN59ZCZ+I18pRRx2V1PLZyvGc9/x5OZ6X+5nPfCap3XXXXSE2/7129OnTJ8QHH3xwUoufYfK/01WrViV5PBOX9iWfjxz/XpI43lIez2Sm/uRzSePfS5PPQ47nqt9///2VbYyalD8Tx/eo/NqSMxMXAAAAAICKsokLAAAAAFBidTNOIT4Stvfeeye1+GjHQw89lNTyI0DUvvxYaXyM+ZRTTklq8dGOtWvXJrUHH3ywAt1RT/IjhR988EGIP218wgEHHBDiWbNmVaA72lI+TuOyyy4LcX7M55577glxvm7gE3PmzEnyefPmhXjQoEFJbfjw4Uneu3fvEMdH5Kl/+bVo7NixIT7wwAOT2sCBA5O8S5cuIT777LOb/XO/+c1vJrXf/va3IR48eHBS69+/f4ifeeaZFjqn0vJ70be+9a0Qx9eMokifrdesWZPUXnrppSR/5ZVXtleL1Jj169cneTyGZejQoUktH5+w7777Vq4x2tz++++f5PnIltjcuXNDvGjRoor1RO36+OOPkzy+D+V7gTvvvHOSx9/B47VWS7yJCwAAAABQYjZxAQAAAABKzCYuAAAAAECJ1exM3HyO03e+850Q5zN2li5dGuK77767so3R5vK1MWrUqBAPGDAgqTU2NoY4n+GVz1qBT/P666+HeMOGDUmtW7duSb7PPvuEOJ/jnM/apfbstttuSR7PAnvxxReT2rJly6rSE7UtnzX42GOPhfi4445LajvssEOS77nnniE2E7f+7Lrrrkke/56IfCbthAkTQpzPievVq1eSx/et+HcIFEX6bP3aa68ltU2bNoV4zJgxSe3oo48OsZm4bWuXXXZJ8jPPPDPE+Szl+Llk4cKFSe2+++5L8nXr1m2vFqkx8feqokjnTX7uc59Lap07d07yPn36hNhzcX2Iv5N/4QtfSGrxfk3+nSn+XSH576yBovjLa038THzCCScktXi+f1Gkz0X5vlGt/G4Sb+ICAAAAAJSYTVwAAAAAgBKr2XEK8ZGLoiiKww8/PMQbN25Mag899FCIX3rppco2RpvLj4Dtt99+Ic5HbaxcuTLEv/rVr5JafBwQtsbixYtD/NFHHyW1HXfcMcmHDBkS4vwoB7Xvq1/9apKvWrUqxFOmTElq+TEy2JL8iNezzz4b4vy5p2fPnkl+8sknh/iFF16oQHdsD/m9oH///iHee++9k9rYsWND/IMf/CCpxUeR8zEc8bHk6dOnJ7V83cTPQfk4hfi/n4/oiI/T9+3bN6nNnj07xPmYIcfwqyserVEU6XiN/Ph6fKT53//935PaPffcU4HuqEX5uonHRy1fvjyp5d/l85zaF9/T4ntGUaTPLYsWLUpqt99+e4h9H2drxNeafCRmfm055JBDQmycAgAAAAAA251NXAAAAACAErOJCwAAAABQYjU7Ezee21QU6cytfCbqK6+8EuJamXNB6+Vz4+JZgF26dElqq1evDvHTTz+d1KwVtlU8Y/m5555LajvssEOS9+jRI8SdO3dOao2NjRXojkqLZ0aeffbZSa1fv34hjmcnQ2vFc5ZXrFiR1PIZcvEzUjwvtSj+coYhbSd/7ojvKYcddlhSi+du53+H8Ty4eJ0URfq7AebPn5/U4jnLRZHOivvrv/7rpPb+++9v8b+X/9wf/vCHpBbP6LX22lZ8XyqKdG3kcwLjecX5bNP4WRpib7/9dojffffdpJb//3/8nJw/F+ezvakN8fPG0KFDk1p8vcnnquf3Jvg08RrK10/+3Nu7d+8Q5/P+a+X3lHgTFwAAAACgxGziAgAAAACUWM2OU8iPYMRHCd97772k9uCDDzb7c9SH+FX4yZMnJ7Vdd901xPkR09dffz3Er776aoW6o72Ij75OmTIlqR1wwAFJ/s4771SjJSooP246cuTIEOcjf6ZPnx7iN954o7KN0S4sWrQoxPG9rCiK4qCDDkry+N5nnELtiI8Q33fffUktHlMwYMCAZmvxfako0qPv+TNRLh7v8OMf/3grOv5LjtqX1/3335/k8ciMfNTCQw89tMW4KFxDaF58LbrqqquS2iWXXJLkL7/8clV6onri542nnnoqqcXjFG644Yaklo+Igk/zwQcfhPiOO+5IapMmTUry+Pk5fyauFbXZNQAAAABAO2ETFwAAAACgxGziAgAAAACUWEM87+pTP9zQsPUfrrAuXbok+eGHHx7ihQsXJrUFCxZUpacqe76pqemItm5ia1Rj3cSzKS+88MKk9r3vfS/E7777blKbOHFiiOfOnVuh7sqjqamp4dM/1fbKdK3ZXvKZhWvWrAnxqlWrktq2XJerwLWm+f9ekp977rkhHj16dFL7+7//+xAvW7asso2Vg3VTYfEcr759+ya1M888M8kffvjhEL/22mtJzfWmdWp13dQp66YV8lmAO+20U4jzObcff/xxiBsbGyvbWPVYN22oV69eSb5hw4YQx/PAS8i6aYUePXokeXy9+fDDD5NaHV1jAt/Bqye/tw0fPjzJ4/W2ePHipFYrz8TexAUAAAAAKDGbuAAAAAAAJVaz4xRwlKOF/16S9+zZM8SrV69OaiV7Zb7iauUoR4cOHZq6du0a8viIVefOnZPPlvzIVT1wrdlK8bUmHpdRFO3vWlPU0Lrp0KFDU6dOnUK+adOmEHfs2DH5bFyjImpm3bT19YaEdUNr1NS6ie9H8XHz/D5Vj0fRS6am1k1b98D/V0vfwePv2hs3bgxx/Kyc16gI4xQAAAAAAGqRTVwAAAAAgBKziQsAAAAAUGKdPv0jUFvy2ZPxbMp2OJeyJjU1NSVzduK/NzNwKSvXmtrU1NSUzBCM/+7MwAWgDDZv3rzF/7kZuMD20tTU1Oyzrxm45eFNXAAAAACAErOJCwAAAABQYsYpUCodO3YMcbdu3ZLa6tWrt+rPaGhoSPLmjh9RbvHfo6PpVFKHDum/Z7b2mmGd1q78vgEAZeK5GKiG+FpjH6WcvIkLAAAAAFBiNnEBAAAAAErMJi4AAAAAQIlt60zcJUVRzK9EI2yzwW3dwDbY6nXT2NgY4q2dgZszJ6pZNbVmNm3a5FpTDjW1bopW3KPMe6qImlo3jY2NrjflUFPrpvBMXBbWDa1RU+tm8+bN1k051NS6KVxvyqCm1oxn4tJodt002PACAAAAACgv4xQAAAAAAErMJi4AAAAAQInZxAUAAAAAKDGbuAAAAAAAJWYTFwAAAACgxGziAgAAAACUmE1cAAAAAIASs4kLAAAAAFBiNnEBAAAAAErs/wHAl0SxDYpb7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(loaders['valid'])\n",
    "images = dataiter.next()\n",
    "if use_cuda:\n",
    "    images = images.cuda()\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# prep images for display\n",
    "images = images.cpu().numpy()\n",
    "batch_size = 32\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.cpu().detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
